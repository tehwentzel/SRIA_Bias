{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5242e5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d450bdf-5923-4b87-8b61-6133cd4d7491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from scipy.ndimage import rotate\n",
    "import Utils\n",
    "from Utils import Constants\n",
    "import cv2\n",
    "from facenet_pytorch import InceptionResnetV1\n",
    "import copy\n",
    "from Models import *\n",
    "from DataLoaders import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb156980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6842, 166), (1711, 166))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels = pd.read_csv('train_data_augmented_balanceddual.csv')\n",
    "validation_labels = pd.read_csv('validation_data_augmented_balanceddual.csv')\n",
    "train_labels.shape, validation_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02e6f6cd-7a43-48de-a6ac-747821eddb8c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model(file,model=None):\n",
    "    if model is None:\n",
    "        model = torch.load(Constants.model_folder + file).to(torch.device('cpu'))\n",
    "    model.load_state_dict(torch.load(Constants.model_folder + file + '_states'))\n",
    "    model.eval()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5bca33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4869, 167)\n",
      "(1209, 167)\n",
      "model being saved to ../../data/models/dualencoder_resnet_flatbias_h400_ed5decodewithrregression__st400_a400_g400_std2_ad2_gd2_rbalanced\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "/opt/conda/envs/rapids/lib/python3.8/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([50])) that is different to the input size (torch.Size([50, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr loss class 22.432090759277344 embed 1.3817708492279053 step 96  | \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.8/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([19])) that is different to the input size (torch.Size([19, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 21.748472165088263 train embed loss 1.6540638755778878 train accuracy [0.056, 0.42, 0.606]\n",
      "val loss 22.112924270629883 val_embed_loss 1.2606768488883973 val accuracy [0.068, 0.504, 0.708] val f1 [0.062, 0.229, 0.332]\n",
      "saved history to ../../data/results/history_dualencoder_resnet_flatbias_h400_ed5decodewithrregression__st400_a400_g400_std2_ad2_gd23.csv\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.8/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9])) that is different to the input size (torch.Size([9, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 21.646439153320934 train embed loss 1.4623081854411535 train accuracy [0.056, 0.5, 0.672]\n",
      "val loss 22.01333869934082 val_embed_loss 1.3007596397399903 val accuracy [0.068, 0.521, 0.747] val f1 [0.062, 0.22, 0.361]\n",
      "saved history to ../../data/results/history_dualencoder_resnet_flatbias_h400_ed5decodewithrregression__st400_a400_g400_std2_ad2_gd23.csv\n",
      "epoch 2\n",
      "curr loss class 19.13224983215332 embed 1.189424753189087 step 78  | | \r"
     ]
    }
   ],
   "source": [
    "def save_train_history(model,history,root=''):\n",
    "    model_name = model.get_identifier()\n",
    "    \n",
    "    df = pd.DataFrame(history)\n",
    "    df['model'] = model_name\n",
    "    string = root + 'results/history_' + model_name + '3.csv'\n",
    "    df.to_csv(string,index=False)\n",
    "    print('saved history to',string)\n",
    "    return df, string\n",
    "\n",
    "def train_model(model,\n",
    "                train_df,\n",
    "                validation_df,\n",
    "                root,\n",
    "                epochs=300,\n",
    "                lr=.001,\n",
    "                batch_size=200,\n",
    "                patience = 20,\n",
    "                loss_weights = [2,1,.5],\n",
    "                save_path=None,\n",
    "                histogram =False,\n",
    "                upsample=False,\n",
    "                random_upsample=True,\n",
    "                softmax_upsample_weights=False,\n",
    "                upsample_validation=False,\n",
    "                random_upsample_validation=False,\n",
    "                embedding_loss_weight = 1,\n",
    "                classification_loss_weight = 1,\n",
    "                starting_loss=100000,\n",
    "                bias_weight_transform=None,\n",
    "                skintone_regression=False,\n",
    "                **kwargs,\n",
    "               ):\n",
    "    pretraining = classification_loss_weight <= .00001\n",
    "    if save_path is None:\n",
    "        save_path = root + 'models/'+ model.get_identifier()\n",
    "        if random_upsample:\n",
    "            \n",
    "            save_path += '_rbalanced'\n",
    "            if softmax_upsample_weights:\n",
    "                save_path += 'soft'\n",
    "        elif upsample:\n",
    "            save_path += '_balanced'\n",
    "        if pretraining:\n",
    "            save_path += '_pretrain'\n",
    "    if upsample:\n",
    "        patience = int(patience/5) + 1\n",
    "        \n",
    "    if bias_weight_transform is not None:\n",
    "        weight_cols = [c for c in train_df.columns if '_bias' in c]\n",
    "        tdf = train_df.copy()\n",
    "        vdf = validation_df.copy()\n",
    "        for col in weight_cols:\n",
    "            tdf[col] = tdf[col].apply(bias_weight_transform)\n",
    "            vdf[col] = vdf[col].apply(bias_weight_transform)\n",
    "        train_df = tdf\n",
    "        validation_df = vdf\n",
    "    train_loader = TripletFaceGenerator(train_df,Constants.data_root,\n",
    "                                 batch_size=batch_size,\n",
    "                                 upsample=upsample,\n",
    "                                 random_upsample=random_upsample,\n",
    "                                 softmax=softmax_upsample_weights,\n",
    "                                 **kwargs)\n",
    "    validation_loader = TripletFaceGenerator(validation_df,Constants.data_root,\n",
    "                                      validation=True,\n",
    "                                      batch_size=batch_size,\n",
    "                                      upsample=upsample_validation,\n",
    "                                     random_upsample=random_upsample_validation,\n",
    "                                     softmax=softmax_upsample_weights,\n",
    "                                      **kwargs)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.train(True)\n",
    "    \n",
    "    cel = torch.nn.CrossEntropyLoss()\n",
    "    format_y = lambda y: y.to(device)\n",
    "    regression_loss = torch.nn.MSELoss()\n",
    "    triplet_loss = torch.nn.TripletMarginLoss()\n",
    "#     embedding_optimizer = torch.optim.Adam(model.encoder.parameters(), lr=lr)\n",
    "#     decoder_optimizer = torch.optim.Adam(model.decoder.parameters(), lr=lr)\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "    \n",
    "    \n",
    "    def format_batch(inputs,grad=True):\n",
    "        xb = []\n",
    "        for xin in inputs:\n",
    "            xin = xin.to(device)\n",
    "            xin.requires_grad_(grad)\n",
    "            xb.append(xin)\n",
    "        return xb\n",
    "    \n",
    "    def embedding_step(m,xbatch): \n",
    "        base = m.encoder(xbatch[0])\n",
    "        positive = m.encoder(xbatch[1])\n",
    "        negative = m.encoder(xbatch[2])\n",
    "        loss = triplet_loss(base,positive,negative)\n",
    "        loss = torch.mul(loss,embedding_loss_weight)\n",
    "        return base,loss\n",
    "    \n",
    "    \n",
    "    def classifier_step(m,embedding,ytrue):\n",
    "        outputs = m.decoder(embedding)\n",
    "        if not skintone_regression:\n",
    "            losses = [cel(ypred.float(),format_y(y.long())) for y,ypred in zip(ytrue,outputs)]\n",
    "            l1 = torch.mul(loss_weights[0],losses[0])\n",
    "            l2 =  torch.mul(loss_weights[1],losses[1])\n",
    "            l3 =  torch.mul(loss_weights[2],losses[2])\n",
    "        else:\n",
    "            l1 = torch.mul( regression_loss(outputs[0].float().view(-1),format_y(ytrue[0].float())), loss_weights[0])\n",
    "            l2 = torch.mul( cel(outputs[1].float(),format_y(ytrue[1].long())), loss_weights[1])\n",
    "            l3 = torch.mul(cel(outputs[2].float(),format_y(ytrue[2].long())), loss_weights[2])\n",
    "        total_losses = l1 + l2 + l3\n",
    "        total_loss = torch.mul(total_losses,classification_loss_weight)\n",
    "        return outputs,total_losses\n",
    "        \n",
    "    def train_epoch(model):\n",
    "        running_loss = 0\n",
    "        running_embed_loss = 0\n",
    "        running_accuracy = [0,0,0]\n",
    "        curr_loss = 0\n",
    "        count = 0\n",
    "        for i, [x_batch, y_batch] in enumerate(train_loader):\n",
    "            x_batch = format_batch(x_batch)\n",
    "            optimizer.zero_grad()\n",
    "            embedding,embedding_loss = embedding_step(model, x_batch)\n",
    "            if pretraining:\n",
    "                classification_loss = embedding_loss - embedding_loss\n",
    "            else:\n",
    "                outputs, classification_loss = classifier_step(model,embedding,y_batch)\n",
    "            total_loss = classification_loss + embedding_loss\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += classification_loss.item()\n",
    "            running_embed_loss += embedding_loss.item()\n",
    "            print('curr loss class',classification_loss.item(),'embed', embedding_loss.item(), 'step',i,' | ',end='\\r')\n",
    "            count += 1\n",
    "            if not pretraining:\n",
    "                with torch.no_grad():\n",
    "                    for i,(y,ypred) in enumerate(zip(y_batch,outputs)):\n",
    "                        accuracy = Utils.categorical_accuracy(ypred.float(),format_y(y))\n",
    "                        running_accuracy[i] += accuracy.item()\n",
    "        return running_loss/count,running_embed_loss/count, [a/count for a in running_accuracy]\n",
    "    \n",
    "    def val_epoch(model):\n",
    "        running_loss = 0\n",
    "        running_embed_loss = 0\n",
    "        running_accuracy = [0,0,0]\n",
    "        running_f1 = [0,0,0]\n",
    "        count = 0\n",
    "        with torch.no_grad():\n",
    "            for i, [x_batch, y_batch] in enumerate(validation_loader):\n",
    "                x_batch = format_batch(x_batch,grad=False)\n",
    "                embedding,embedding_loss = embedding_step(model, x_batch)\n",
    "                if pretraining:\n",
    "                    classification_loss = embedding_loss - embedding_loss\n",
    "                else:\n",
    "                    outputs, classification_loss = classifier_step(model,embedding,y_batch)\n",
    "                running_loss += classification_loss.item()\n",
    "                running_embed_loss += embedding_loss.item()\n",
    "                count += 1\n",
    "                if not pretraining:\n",
    "                    for i,(y,ypred) in enumerate(zip(y_batch, outputs)):\n",
    "                        accuracy = Utils.categorical_accuracy(ypred.float(),format_y(y))\n",
    "                        f1, precision, recall = Utils.macro_f1(torch.argmax(ypred.float(),axis=1),format_y(y))\n",
    "                        running_accuracy[i] += accuracy.item()\n",
    "                        running_f1[i] += f1.item()\n",
    "        return running_loss/count,running_embed_loss/count, [a/count for a in running_accuracy], [f/count for f in running_f1]\n",
    "    shorten = lambda array: [np.round(a, 3) for a in array]\n",
    "    \n",
    "    best_val_loss = starting_loss\n",
    "    steps_since_improvement = 0\n",
    "    hist = []\n",
    "    best_weights = model.state_dict()\n",
    "    print('model being saved to',save_path)\n",
    "    for epoch in range(epochs):\n",
    "        print('epoch',epoch)\n",
    "        model.train()\n",
    "        avg_loss,avg_embed_loss, avg_acc = train_epoch(model)\n",
    "        print('train loss', avg_loss,'train embed loss',avg_embed_loss, 'train accuracy', shorten(avg_acc))\n",
    "        model.eval()\n",
    "        val_loss,val_embed_loss, val_acc, val_f1 = val_epoch(model)\n",
    "        if pretraining:\n",
    "            val_loss = val_embed_loss\n",
    "        print('val loss', val_loss, 'val_embed_loss', val_embed_loss, \n",
    "              'val accuracy', shorten(val_acc), 'val f1', shorten(val_f1))\n",
    "        #don't save immediately in case I cancel training\n",
    "        if best_val_loss > val_loss and epoch > 1:\n",
    "            torch.save(model,save_path)\n",
    "            torch.save(model.state_dict(),save_path+'_states')\n",
    "            print('saving model')\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "            best_val_loss = val_loss\n",
    "            steps_since_improvement = 0\n",
    "        else:\n",
    "            steps_since_improvement += 1\n",
    "        \n",
    "        hist_entry = {\n",
    "            'epoch': epoch,\n",
    "            'train_loss': avg_loss,\n",
    "            'train_acc':avg_acc,\n",
    "            'val_loss':val_loss,\n",
    "            'val_acc': val_acc,\n",
    "            'lr': lr,\n",
    "            'loss_weights': '_'.join([str(l) for l in loss_weights])\n",
    "        }\n",
    "        hist.append(hist_entry)\n",
    "        save_train_history(model,hist,root=root)\n",
    "        if steps_since_improvement > patience:\n",
    "            break\n",
    "    model.load_state_dict(best_weights)\n",
    "    return model,hist\n",
    "\n",
    "    \n",
    "model = TripletModel2(\n",
    "    encoder=TripletFacenetEncoder(base_model=ResNet18(),base_name='resnet_flatbias',embedding_dropout=.5),\n",
    ")\n",
    "\n",
    "m,h = train_model(\n",
    "    model,\n",
    "    train_labels,\n",
    "    validation_labels,\n",
    "    Constants.data_root,\n",
    "    batch_size=50,\n",
    "    embedding_loss_weight=1,\n",
    "    classification_loss_weight=.5,\n",
    "    lr=.0001,\n",
    "    skintone_regression=True,\n",
    ")\n",
    "del m\n",
    "h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2c1bc5-44fa-4467-b4b5-852b336c744b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae18dec9-1e27-4dbc-af0c-4787f12a4dc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
