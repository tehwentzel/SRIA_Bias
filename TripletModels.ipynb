{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5242e5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d450bdf-5923-4b87-8b61-6133cd4d7491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from scipy.ndimage import rotate\n",
    "import Utils\n",
    "from Utils import Constants\n",
    "import cv2\n",
    "from facenet_pytorch import InceptionResnetV1\n",
    "import copy\n",
    "from Models import *\n",
    "from DataLoaders import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb156980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6842, 166), (1711, 166))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels = pd.read_csv('train_data_augmented_balanceddual.csv')\n",
    "validation_labels = pd.read_csv('validation_data_augmented_balanceddual.csv')\n",
    "train_labels.shape, validation_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02e6f6cd-7a43-48de-a6ac-747821eddb8c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model(file,model=None):\n",
    "    if model is None:\n",
    "        model = torch.load(Constants.model_folder + file).to(torch.device('cpu'))\n",
    "    model.load_state_dict(torch.load(Constants.model_folder + file + '_states'))\n",
    "    model.eval()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5bca33",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4869, 167)\n",
      "(1209, 167)\n",
      "model being saved to ../../data/models/gridsearch3_smalldualencoder_dualfacenet_h400_ed3triplet_decoder__st100_a40_g20_std2_ad2_gd2_rbalancedsoftembed0.0_lw2-1-1_noise01\n",
      "epoch 0\n",
      "train loss 6.522767855196583 train embed loss 0.0 train accuracy [0.148, 0.421, 0.66]\n",
      "val loss 6.1657544708251955 val_embed_loss 0.0 val accuracy [0.182, 0.604, 0.846] val f1 [0.074, 0.298, 0.42] disparities [0.5188726186752319, 0.738842248916626, 0.11218416690826416] score 4.355351685586682\n",
      "saved history to ../../data/results/history_gridsearch3_smalldualencoder_dualfacenet_h400_ed3triplet_decoder__st100_a40_g20_std2_ad2_gd23.csv\n",
      "epoch 1\n",
      "train loss 6.291047436850412 train embed loss 0.0 train accuracy [0.2, 0.538, 0.713]\n",
      "val loss 6.101686420440674 val_embed_loss 0.0 val accuracy [0.221, 0.624, 0.78] val f1 [0.102, 0.288, 0.377] disparities [0.47208917140960693, 0.8399781584739685, 0.30558860301971436] score 3.972605734938504\n",
      "saved history to ../../data/results/history_gridsearch3_smalldualencoder_dualfacenet_h400_ed3triplet_decoder__st100_a40_g20_std2_ad2_gd23.csv\n",
      "epoch 2\n",
      "train loss 6.2333872123640415 train embed loss 0.0 train accuracy [0.208, 0.55, 0.719]\n",
      "val loss 5.980448760986328 val_embed_loss 0.0 val accuracy [0.256, 0.642, 0.826] val f1 [0.122, 0.296, 0.406] disparities [0.4481815993785858, 0.8473419547080994, 0.1387600302696228] score 4.661936173507316\n",
      "saving model\n",
      "saved history to ../../data/results/history_gridsearch3_smalldualencoder_dualfacenet_h400_ed3triplet_decoder__st100_a40_g20_std2_ad2_gd23.csv\n",
      "epoch 3\n",
      "train loss 6.166438890963184 train embed loss 0.0 train accuracy [0.235, 0.553, 0.733]\n",
      "val loss 5.951827011108398 val_embed_loss 0.0 val accuracy [0.267, 0.63, 0.846] val f1 [0.136, 0.287, 0.42] disparities [0.4527207911014557, 0.8357937932014465, 0.03552764654159546] score 5.010396746824397\n",
      "saving model\n",
      "saved history to ../../data/results/history_gridsearch3_smalldualencoder_dualfacenet_h400_ed3triplet_decoder__st100_a40_g20_std2_ad2_gd23.csv\n",
      "epoch 4\n",
      "train loss 6.141013067595813 train embed loss 0.0 train accuracy [0.238, 0.566, 0.729]\n",
      "val loss 5.910690650939942 val_embed_loss 0.0 val accuracy [0.268, 0.647, 0.865] val f1 [0.136, 0.299, 0.43] disparities [0.5109800696372986, 0.8701359629631042, 0.07960492372512817] score 4.80484394987563\n",
      "saved history to ../../data/results/history_gridsearch3_smalldualencoder_dualfacenet_h400_ed3triplet_decoder__st100_a40_g20_std2_ad2_gd23.csv\n",
      "epoch 5\n",
      "train loss 6.1162450167597555 train embed loss 0.0 train accuracy [0.251, 0.578, 0.739]\n",
      "val loss 5.889362735748291 val_embed_loss 0.0 val accuracy [0.271, 0.65, 0.872] val f1 [0.141, 0.295, 0.433] disparities [0.4375207722187042, 0.8623947501182556, 0.015897274017333984] score 5.045012685223206\n",
      "saving model\n",
      "saved history to ../../data/results/history_gridsearch3_smalldualencoder_dualfacenet_h400_ed3triplet_decoder__st100_a40_g20_std2_ad2_gd23.csv\n",
      "epoch 6\n",
      "train loss 6.104064114239751 train embed loss 0.0 train accuracy [0.249, 0.57, 0.748]\n",
      "val loss 5.8459238433837895 val_embed_loss 0.0 val accuracy [0.274, 0.665, 0.886] val f1 [0.142, 0.309, 0.441] disparities [0.4470987319946289, 0.8642808198928833, 0.016318857669830322] score 5.108090514222562\n",
      "saving model\n",
      "saved history to ../../data/results/history_gridsearch3_smalldualencoder_dualfacenet_h400_ed3triplet_decoder__st100_a40_g20_std2_ad2_gd23.csv\n",
      "epoch 7\n",
      "train loss 6.085852277522185 train embed loss 0.0 train accuracy [0.254, 0.582, 0.759]\n",
      "val loss 5.869285430908203 val_embed_loss 0.0 val accuracy [0.28, 0.652, 0.858] val f1 [0.148, 0.301, 0.427] disparities [0.48023146390914917, 0.8500379920005798, 0.11451995372772217] score 4.973702591392435\n",
      "saved history to ../../data/results/history_gridsearch3_smalldualencoder_dualfacenet_h400_ed3triplet_decoder__st100_a40_g20_std2_ad2_gd23.csv\n",
      "epoch 8\n",
      "train loss 6.067943816282312 train embed loss 0.0 train accuracy [0.261, 0.581, 0.758]\n",
      "val loss 5.8292333602905275 val_embed_loss 0.0 val accuracy [0.283, 0.651, 0.877] val f1 [0.146, 0.306, 0.436] disparities [0.4784768521785736, 0.867072582244873, 0.036612510681152344] score 5.098734808390531\n",
      "saved history to ../../data/results/history_gridsearch3_smalldualencoder_dualfacenet_h400_ed3triplet_decoder__st100_a40_g20_std2_ad2_gd23.csv\n",
      "epoch 9\n",
      "train loss 6.045045385555345 train embed loss 0.0 train accuracy [0.268, 0.588, 0.759]\n",
      "val loss 5.825658550262451 val_embed_loss 0.0 val accuracy [0.282, 0.652, 0.886] val f1 [0.152, 0.302, 0.44] disparities [0.39106735587120056, 0.8630120158195496, 0.056128740310668945] score 5.1370043455244225\n",
      "saving model\n",
      "saved history to ../../data/results/history_gridsearch3_smalldualencoder_dualfacenet_h400_ed3triplet_decoder__st100_a40_g20_std2_ad2_gd23.csv\n",
      "epoch 10\n",
      "train loss 6.044751916612897 train embed loss 0.0 train accuracy [0.267, 0.586, 0.765]\n",
      "val loss 5.81115800857544 val_embed_loss 0.0 val accuracy [0.303, 0.654, 0.874] val f1 [0.159, 0.289, 0.434] disparities [0.47709032893180847, 0.9483306407928467, 0.02194732427597046] score 4.9307666522304325\n",
      "saved history to ../../data/results/history_gridsearch3_smalldualencoder_dualfacenet_h400_ed3triplet_decoder__st100_a40_g20_std2_ad2_gd23.csv\n",
      "epoch 11\n",
      "train loss 6.022107537911863 train embed loss 0.0 train accuracy [0.273, 0.591, 0.778]\n",
      "val loss 5.81632022857666 val_embed_loss 0.0 val accuracy [0.295, 0.652, 0.868] val f1 [0.153, 0.29, 0.429] disparities [0.4502372741699219, 0.922249436378479, 0.0860215425491333] score 4.875307328073658\n",
      "saved history to ../../data/results/history_gridsearch3_smalldualencoder_dualfacenet_h400_ed3triplet_decoder__st100_a40_g20_std2_ad2_gd23.csv\n",
      "epoch 12\n",
      "train loss 6.008729282690554 train embed loss 0.0 train accuracy [0.276, 0.599, 0.776]\n",
      "val loss 5.803952960968018 val_embed_loss 0.0 val accuracy [0.303, 0.652, 0.863] val f1 [0.163, 0.299, 0.429] disparities [0.4050954282283783, 0.8709644675254822, 0.013588368892669678] score 5.324716749261187\n",
      "saving model\n",
      "saved history to ../../data/results/history_gridsearch3_smalldualencoder_dualfacenet_h400_ed3triplet_decoder__st100_a40_g20_std2_ad2_gd23.csv\n",
      "epoch 13\n",
      "train loss 6.011524433992347 train embed loss 0.0 train accuracy [0.274, 0.6, 0.768]\n",
      "val loss 5.778911037445068 val_embed_loss 0.0 val accuracy [0.316, 0.666, 0.878] val f1 [0.166, 0.31, 0.436] disparities [0.4193955063819885, 0.8979344964027405, 0.07389706373214722] score 5.259730401097768\n",
      "saved history to ../../data/results/history_gridsearch3_smalldualencoder_dualfacenet_h400_ed3triplet_decoder__st100_a40_g20_std2_ad2_gd23.csv\n",
      "epoch 14\n",
      "train loss 5.986664426570036 train embed loss 0.0 train accuracy [0.281, 0.597, 0.779]\n",
      "val loss 5.796925029754639 val_embed_loss 0.0 val accuracy [0.296, 0.658, 0.892] val f1 [0.161, 0.305, 0.443] disparities [0.3940255343914032, 0.8940760493278503, 0.07845991849899292] score 5.105620814531231\n",
      "saved history to ../../data/results/history_gridsearch3_smalldualencoder_dualfacenet_h400_ed3triplet_decoder__st100_a40_g20_std2_ad2_gd23.csv\n",
      "epoch 15\n",
      "train loss 5.975498340567764 train embed loss 0.0 train accuracy [0.289, 0.593, 0.779]\n",
      "val loss 5.79194372177124 val_embed_loss 0.0 val accuracy [0.297, 0.664, 0.889] val f1 [0.15, 0.304, 0.443] disparities [0.4339958429336548, 0.8904128670692444, 0.005758464336395264] score 5.238198648101002\n",
      "saved history to ../../data/results/history_gridsearch3_smalldualencoder_dualfacenet_h400_ed3triplet_decoder__st100_a40_g20_std2_ad2_gd23.csv\n",
      "epoch 16\n",
      "train loss 5.979650609347285 train embed loss 0.0 train accuracy [0.289, 0.602, 0.768]\n",
      "val loss 5.76462537765503 val_embed_loss 0.0 val accuracy [0.323, 0.669, 0.882] val f1 [0.165, 0.313, 0.438] disparities [0.46970170736312866, 0.8826561570167542, 0.005971729755401611] score 5.496628808052106\n",
      "saving model\n",
      "saved history to ../../data/results/history_gridsearch3_smalldualencoder_dualfacenet_h400_ed3triplet_decoder__st100_a40_g20_std2_ad2_gd23.csv\n",
      "epoch 17\n",
      "train loss 5.9593265689149195 train embed loss 0.0 train accuracy [0.292, 0.605, 0.786]\n",
      "val loss 5.752773704528809 val_embed_loss 0.0 val accuracy [0.322, 0.667, 0.887] val f1 [0.171, 0.307, 0.441] disparities [0.5329961180686951, 0.9172651171684265, 0.04640519618988037] score 5.192724396044947\n",
      "saved history to ../../data/results/history_gridsearch3_smalldualencoder_dualfacenet_h400_ed3triplet_decoder__st100_a40_g20_std2_ad2_gd23.csv\n",
      "epoch 18\n",
      "curr loss class 6.121708393096924 embed 0.0 step 9  |  \r"
     ]
    }
   ],
   "source": [
    "def save_train_history(model,history,root=''):\n",
    "    model_name = model.get_identifier()\n",
    "    \n",
    "    df = pd.DataFrame(history)\n",
    "    df['model'] = model_name\n",
    "    string = root + 'results/history_' + model_name + '3.csv'\n",
    "    df.to_csv(string,index=False)\n",
    "    print('saved history to',string)\n",
    "    return df, string\n",
    "\n",
    "def subclass_accuracy(ypred,y,nclass=None):\n",
    "    if nclass is None:\n",
    "        nclass = len(torch.unique(y))\n",
    "        classes = torch.unique(y)\n",
    "    else:\n",
    "        classes = torch.Tensor([i for i in range(nclass)]).float()\n",
    "    results = torch.zeros(nclass)\n",
    "    if ypred.ndim > 1:\n",
    "        ypred = torch.argmax(ypred,1).long()\n",
    "    for i,c in enumerate(torch.unique(y)):\n",
    "        yy = (y == c)\n",
    "        yypred = (ypred == c)\n",
    "        good = torch.logical_and(yy,yypred).float().sum()/yy.float().sum()\n",
    "        results[i] = good\n",
    "    return results\n",
    "\n",
    "def comp_score(accs,disp,csizes):\n",
    "    score = 0\n",
    "    for acc,disp,size in zip(accs,disp,csizes):\n",
    "        score += size*(acc)*(1-disp**(size/2))\n",
    "    return score\n",
    "\n",
    "def train_model(model,\n",
    "                train_df,\n",
    "                validation_df,\n",
    "                root,\n",
    "                epochs=300,\n",
    "                lr=.001,\n",
    "                batch_size=200,\n",
    "                patience = 20,\n",
    "                loss_weights = [2,1,.5],\n",
    "                save_path=None,\n",
    "                histogram =False,\n",
    "                upsample=False,\n",
    "                random_upsample=True,\n",
    "                softmax_upsample_weights=False,\n",
    "                upsample_validation=False,\n",
    "                random_upsample_validation=False,\n",
    "                embedding_loss_weight = 1,\n",
    "                classification_loss_weight = 1,\n",
    "                starting_loss=100000,\n",
    "                bias_weight_transform=None,\n",
    "                skintone_regression=False,\n",
    "                noise=.05,\n",
    "                skintone_patch_anchor_prob=0,\n",
    "                run_name = '',\n",
    "                **kwargs,\n",
    "               ):\n",
    "    pretraining = classification_loss_weight <= .00001\n",
    "    if save_path is None:\n",
    "        save_path = root + 'models/'+ run_name + model.get_identifier() \n",
    "        if random_upsample:\n",
    "            \n",
    "            save_path += '_rbalanced'\n",
    "            if softmax_upsample_weights:\n",
    "                save_path += 'soft'\n",
    "        elif upsample:\n",
    "            save_path += '_balanced'\n",
    "        if pretraining:\n",
    "            save_path += '_pretrain'\n",
    "        else:\n",
    "            save_path += 'embed' + str(np.round(embedding_loss_weight/classification_loss_weight,2))\n",
    "    if skintone_patch_anchor_prob > 0:\n",
    "        save_path += '_spab'+str(skintone_patch_anchor_prob).replace('.','')\n",
    "    save_path += '_lw' + '-'.join([str(l) for l in loss_weights])\n",
    "    save_path += '_noise' + str(noise).replace('0.','').replace('.','')\n",
    "    if upsample:\n",
    "        patience = int(patience/5) + 1\n",
    "    class_sizes = [10,4,2]\n",
    "    if bias_weight_transform is not None:\n",
    "        weight_cols = [c for c in train_df.columns if '_bias' in c]\n",
    "        tdf = train_df.copy()\n",
    "        vdf = validation_df.copy()\n",
    "        for col in weight_cols:\n",
    "            tdf[col] = tdf[col].apply(bias_weight_transform)\n",
    "            vdf[col] = vdf[col].apply(bias_weight_transform)\n",
    "        train_df = tdf\n",
    "        validation_df = vdf\n",
    "    train_loader = TripletFaceGenerator(train_df,Constants.data_root,\n",
    "                                 batch_size=batch_size,\n",
    "                                 upsample=upsample,\n",
    "                                 random_upsample=random_upsample,\n",
    "                                 skintone_patch_anchor_prob=skintone_patch_anchor_prob,\n",
    "                                 softmax=softmax_upsample_weights,\n",
    "                                 noise_sigma=noise,\n",
    "                                 **kwargs)\n",
    "    validation_loader = TripletFaceGenerator(validation_df,Constants.data_root,\n",
    "                                      validation=True,\n",
    "                                      batch_size=batch_size,\n",
    "                                      upsample=upsample_validation,\n",
    "                                     random_upsample=random_upsample_validation,\n",
    "                                     softmax=softmax_upsample_weights,\n",
    "                                     skintone_patch_anchor_prob=0,\n",
    "                                     noise_sigma=noise,\n",
    "                                      **kwargs)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.train(True)\n",
    "    \n",
    "    cel = torch.nn.CrossEntropyLoss()\n",
    "    format_y = lambda y: y.to(device)\n",
    "    regression_loss = torch.nn.MSELoss()\n",
    "    triplet_loss = torch.nn.TripletMarginLoss()\n",
    "#     embedding_optimizer = torch.optim.Adam(model.encoder.parameters(), lr=lr)\n",
    "#     decoder_optimizer = torch.optim.Adam(model.decoder.parameters(), lr=lr)\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "    \n",
    "    \n",
    "    def format_batch(inputs,grad=True):\n",
    "        xb = []\n",
    "        for xin in inputs:\n",
    "            xin = xin.to(device)\n",
    "            xin.requires_grad_(grad)\n",
    "            xb.append(xin)\n",
    "        return xb\n",
    "    \n",
    "    def embedding_step(m,xbatch): \n",
    "        base = m.encoder(xbatch[0])\n",
    "        positive = m.encoder(xbatch[1])\n",
    "        negative = m.encoder(xbatch[2])\n",
    "        loss = triplet_loss(base,positive,negative)\n",
    "        loss = torch.mul(loss,embedding_loss_weight)\n",
    "        return base,loss\n",
    "    \n",
    "    \n",
    "    def classifier_step(m,embedding,ytrue):\n",
    "        outputs = m.decoder(embedding)\n",
    "        if not skintone_regression:\n",
    "            losses = [cel(ypred.float(),format_y(y.long())) for y,ypred in zip(ytrue,outputs)]\n",
    "            l1 = torch.mul(loss_weights[0],losses[0])\n",
    "            l2 =  torch.mul(loss_weights[1],losses[1])\n",
    "            l3 =  torch.mul(loss_weights[2],losses[2])\n",
    "        else:\n",
    "            l1 = torch.mul( regression_loss(outputs[0].float().view(-1),format_y(ytrue[0].float())), loss_weights[0])\n",
    "            l2 = torch.mul( cel(outputs[1].float(),format_y(ytrue[1].long())), loss_weights[1])\n",
    "            l3 = torch.mul(cel(outputs[2].float(),format_y(ytrue[2].long())), loss_weights[2])\n",
    "        total_losses = l1 + l2 + l3\n",
    "        total_loss = torch.mul(total_losses,classification_loss_weight)\n",
    "        return outputs,total_losses\n",
    "        \n",
    "    def train_epoch(model):\n",
    "        running_loss = 0\n",
    "        running_embed_loss = 0\n",
    "        running_accuracy = [0,0,0]\n",
    "        curr_loss = 0\n",
    "        count = 0\n",
    "        for i, [x_batch, y_batch] in enumerate(train_loader):\n",
    "            x_batch = format_batch(x_batch)\n",
    "            optimizer.zero_grad()\n",
    "            embedding,embedding_loss = embedding_step(model, x_batch)\n",
    "            if pretraining:\n",
    "                classification_loss = embedding_loss - embedding_loss\n",
    "            else:\n",
    "                outputs, classification_loss = classifier_step(model,embedding,y_batch)\n",
    "            total_loss = classification_loss + embedding_loss\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += classification_loss.item()\n",
    "            running_embed_loss += embedding_loss.item()\n",
    "            print('curr loss class',classification_loss.item(),'embed', embedding_loss.item(), 'step',i,' | ',end='\\r')\n",
    "            count += 1\n",
    "            if not pretraining:\n",
    "                with torch.no_grad():\n",
    "                    for i,(y,ypred) in enumerate(zip(y_batch,outputs)):\n",
    "                        accuracy = Utils.categorical_accuracy(ypred.float(),format_y(y))\n",
    "                        running_accuracy[i] += accuracy.item()\n",
    "        return running_loss/count,running_embed_loss/count, [a/count for a in running_accuracy]\n",
    "    \n",
    "    def val_epoch(model):\n",
    "        running_loss = 0\n",
    "        running_embed_loss = 0\n",
    "        running_accuracy = [0,0,0]\n",
    "        running_f1 = [0,0,0]\n",
    "        running_subclass_acc = [None,None,None]\n",
    "        count = 0\n",
    "        with torch.no_grad():\n",
    "            for i, [x_batch, y_batch] in enumerate(validation_loader):\n",
    "                x_batch = format_batch(x_batch,grad=False)\n",
    "                embedding,embedding_loss = embedding_step(model, x_batch)\n",
    "                if pretraining:\n",
    "                    classification_loss = embedding_loss - embedding_loss\n",
    "                else:\n",
    "                    outputs, classification_loss = classifier_step(model,embedding,y_batch)\n",
    "                running_loss += classification_loss.item()\n",
    "                running_embed_loss += embedding_loss.item()\n",
    "                count += 1\n",
    "                if not pretraining:\n",
    "                    for i,(y,ypred) in enumerate(zip(y_batch, outputs)):\n",
    "                        accuracy = Utils.categorical_accuracy(ypred.float(),format_y(y))\n",
    "                        f1, precision, recall = Utils.macro_f1(torch.argmax(ypred.float(),axis=1),format_y(y))\n",
    "                        subclass_acc = subclass_accuracy(ypred.float(),format_y(y),class_sizes[i])\n",
    "                        if running_subclass_acc[i] is None:\n",
    "                            running_subclass_acc[i] = subclass_acc\n",
    "                        else:\n",
    "                            running_subclass_acc[i] += subclass_acc\n",
    "                        running_accuracy[i] += accuracy.item()\n",
    "                        running_f1[i] += f1.item()\n",
    "        running_subclass_acc = [torch.mul(r,1/count) for r in running_subclass_acc]\n",
    "        disparities = [(torch.max(v) - torch.min(v)).item() for v in running_subclass_acc]\n",
    "        return running_loss/count,running_embed_loss/count, [a/count for a in running_accuracy], [f/count for f in running_f1],disparities\n",
    "    shorten = lambda array: [np.round(a, 3) for a in array]\n",
    "    \n",
    "    best_val_res = {}\n",
    "    best_val_loss=1000000\n",
    "    best_score = 0\n",
    "    steps_since_improvement = 0\n",
    "    hist = []\n",
    "    best_weights = model.state_dict()\n",
    "    print('model being saved to',save_path)\n",
    "    for epoch in range(epochs):\n",
    "        print('epoch',epoch)\n",
    "        model.train()\n",
    "        avg_loss,avg_embed_loss, avg_acc = train_epoch(model)\n",
    "        print('train loss', avg_loss,'train embed loss',avg_embed_loss, 'train accuracy', shorten(avg_acc))\n",
    "        model.eval()\n",
    "        val_loss,val_embed_loss, val_acc, val_f1, val_disp = val_epoch(model)\n",
    "        val_score = comp_score(val_acc,val_disp,class_sizes)\n",
    "        if pretraining:\n",
    "            val_loss = val_embed_loss\n",
    "        print('val loss', val_loss, 'val_embed_loss', val_embed_loss, \n",
    "              'val accuracy', shorten(val_acc), 'val f1', shorten(val_f1),\n",
    "                 'disparities',val_disp, 'score', val_score,\n",
    "             )\n",
    "        #don't save immediately in case I cancel training\n",
    "        if best_score < val_score and epoch > 1:\n",
    "            torch.save(model,save_path)\n",
    "            torch.save(model.state_dict(),save_path+'_states')\n",
    "            print('saving model')\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "            best_val_loss = val_loss\n",
    "            best_score = val_score\n",
    "            best_val_res = {'accuracy': val_acc, 'f1': val_f1, 'disparity': val_disp,'score':best_score,'loss':val_loss}\n",
    "            steps_since_improvement = 0\n",
    "        else:\n",
    "            steps_since_improvement += 1\n",
    "        \n",
    "        hist_entry = {\n",
    "            'epoch': epoch,\n",
    "            'train_loss': avg_loss,\n",
    "            'train_acc':avg_acc,\n",
    "            'val_loss':val_loss,\n",
    "            'val_acc': val_acc,\n",
    "            'score': val_score,\n",
    "            'disp': val_disp,\n",
    "            'lr': lr,\n",
    "            'loss_weights': '_'.join([str(l) for l in loss_weights])\n",
    "        }\n",
    "        hist.append(hist_entry)\n",
    "        save_train_history(model,hist,root=root)\n",
    "        if steps_since_improvement > patience:\n",
    "            break\n",
    "    model.load_state_dict(best_weights)\n",
    "    return model,hist,best_val_res,save_path\n",
    "\n",
    "    \n",
    "models = [\n",
    "#      lambda : TripletModel2(encoder=TripletFacenetEncoder(embedding_dropout=.1),decoder=TripletFacenetClassifier(400,st_dims=[20],age_dims=[8],gender_dims=[4]),name='gridsearch_sm0ll'),\n",
    "#     lambda : TripletModel2(encoder=TripletFacenetEncoder(embedding_dropout=.1),decoder=TripletFacenetClassifier(400,st_dims=[100],age_dims=[40],gender_dims=[20]),name='gridsearch_small'),\n",
    "#     lambda : TripletModel2(encoder=TripletFacenetEncoder(embedding_dropout=.3),decoder=TripletFacenetClassifier(400,st_dims=[20],age_dims=[8],gender_dims=[4]),name='gridsearch_sm0ll'),\n",
    "    lambda : TripletModel2(encoder=TripletFacenetEncoder(embedding_dropout=.3),decoder=TripletFacenetClassifier(400,st_dims=[100],age_dims=[40],gender_dims=[20]),name='gridsearch3_small'),\n",
    "    lambda : TripletModel2(encoder=TripletFacenetEncoder(embedding_dropout=.3),decoder=TripletFacenetClassifier(400,st_dims=[100],age_dims=[100],gender_dims=[100]),name='gridsearch3_medium'),\n",
    "#     lambda : TripletModel2(encoder=TripletFacenetEncoder(embedding_dropout=.6),decoder=TripletFacenetClassifier(400,st_dims=[20],age_dims=[8],gender_dims=[4]),name='gridsearch_sm0ll'),\n",
    "#     lambda : TripletModel2(encoder=TripletFacenetEncoder(embedding_dropout=.6),decoder=TripletFacenetClassifier(400,st_dims=[100],age_dims=[40],gender_dims=[20]),name='gridsearch_small'),\n",
    "#     lambda : TripletModel2(encoder=TripletFacenetEncoder(embedding_dropout=.6),decoder=TripletFacenetClassifier(400,st_dims=[100],age_dims=[100],gender_dims=[100]),name='gridsearch_medium'),\n",
    "        lambda : TripletModel2(encoder=TripletFacenetEncoder(),name='gridsearch_baseline'),\n",
    "#     lambda : TripletModel2(encoder=TripletFacenetEncoder(base_model=ResNet18()),decoder=TripletFacenetClassifier(400,st_dims=[100],age_dims=[40],gender_dims=[20]),name='gridsearch3_resnetsmall'),\n",
    "    lambda : TripletModel2(encoder=SimpleEncoder(),name='gridsearch_simple'),\n",
    "#     lambda : TripletModel2(encoder=FrozenDualEncoder(),name='gridsearch_frozen'),\n",
    "#     lambda : TripletModel2(encoder=SimpleEncoder(fine_tune=False),name='gridsearch_simplefrozen'),\n",
    "]\n",
    "\n",
    "results = []\n",
    "for embed_loss_weight in [0,.1]:\n",
    "    for stap in [0]:\n",
    "        for noise in [.01]:\n",
    "            for loss_weights in [[2,1,1]]:\n",
    "                for model in models:\n",
    "                    for wt in ['softmax']:\n",
    "                        if embed_loss_weight == 0 and wt == 'flat':\n",
    "                            continue\n",
    "                        softmax = (wt == 'softmax')\n",
    "                        bias_weight_transform=None\n",
    "                        if wt == 'flat':\n",
    "                            bias_weight_transform = lambda x: int(x > 0)\n",
    "                        try:\n",
    "                            m,h,entry,mname = train_model(\n",
    "                                model(),\n",
    "                                train_labels,\n",
    "                                validation_labels,\n",
    "                                Constants.data_root,\n",
    "                                batch_size=50,\n",
    "                                embedding_loss_weight=embed_loss_weight,\n",
    "                                classification_loss_weight=1 - embed_loss_weight,\n",
    "                                lr=.0001,\n",
    "                                patience=5,\n",
    "                                softmax_upsample_weights=softmax,\n",
    "                                loss_weights=loss_weights,\n",
    "                                skintone_patch_anchor_prob=stap,\n",
    "                                skintone_regression=False,\n",
    "                                noise=noise,\n",
    "                            )\n",
    "                            entry['noise'] = noise\n",
    "                            entry['embed_weight'] = embed_loss_weight\n",
    "                            entry['model'] = mname\n",
    "                            entry['st_patch_prob'] = stap\n",
    "                            entry['softmax'] = softmax\n",
    "                            entry['weight_type'] = wt\n",
    "                            entry['loss_weights'] = loss_weights\n",
    "                            results.append(entry)\n",
    "                            print('___________')\n",
    "                            print(entry['score'],\n",
    "                                  entry['accuracy'],\n",
    "                                  entry['model'],\n",
    "                                  entry['embed_weight'],\n",
    "                                  wt,\n",
    "                                 )\n",
    "                            print('______________')\n",
    "                            pd.DataFrame(results).to_csv(Constants.result_folder + '_weight_gridsearch_presentation2.csv')\n",
    "                        except Exception as e:\n",
    "                            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7e1486-f1e6-4e48-92f0-9e3e72f1896b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results).sort_values('score',ascending=False).model.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbcaf49-3a9c-45c1-93e7-888fef3dbc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = TripletModel2(encoder=TripletFacenetEncoder(),name='gridsearch_baseline')\n",
    "test.encoder.embedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a69f57-285b-4807-8b1f-c1f5217a6a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('lol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427653cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
