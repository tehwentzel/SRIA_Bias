{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5242e5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d450bdf-5923-4b87-8b61-6133cd4d7491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from scipy.ndimage import rotate\n",
    "import Utils\n",
    "from Utils import Constants\n",
    "import cv2\n",
    "from facenet_pytorch import InceptionResnetV1\n",
    "import copy\n",
    "from Models import *\n",
    "from DataLoaders import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb156980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6842, 166), (1711, 166))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels = pd.read_csv('train_data_augmented_balanceddual.csv')\n",
    "validation_labels = pd.read_csv('validation_data_augmented_balanceddual.csv')\n",
    "train_labels.shape, validation_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02e6f6cd-7a43-48de-a6ac-747821eddb8c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model(file,model=None):\n",
    "    if model is None:\n",
    "        model = torch.load(Constants.model_folder + file).to(torch.device('cpu'))\n",
    "    model.load_state_dict(torch.load(Constants.model_folder + file + '_states'))\n",
    "    model.eval()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5bca33",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4869, 167)\n",
      "(1209, 167)\n",
      "model being saved to ../../data/models/gridsearch3_smalldualencoder_dualfacenet_h400_ed3triplet_decoder__st100_a40_g20_std2_ad2_gd2_rbalancedembed0.0_lw2-1-1_noise01\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 6.5542988923131205 train embed loss 0.0 train accuracy [0.138, 0.425, 0.61]\n",
      "val loss 6.215064296722412 val_embed_loss 0.0 val accuracy [0.184, 0.592, 0.783] val f1 [0.08, 0.261, 0.385] disparities [0.6415237784385681, 0.801367461681366, 0.31263816356658936] score 3.559762276763947\n",
      "saved history to ../../data/results/history_gridsearch3_smalldualencoder_dualfacenet_h400_ed3triplet_decoder__st100_a40_g20_std2_ad2_gd23.csv\n",
      "epoch 1\n",
      "train loss 6.3194454786728835 train embed loss 0.0 train accuracy [0.192, 0.518, 0.709]\n",
      "val loss 6.035549926757812 val_embed_loss 0.0 val accuracy [0.225, 0.631, 0.87] val f1 [0.108, 0.309, 0.433] disparities [0.5854372382164001, 0.764141321182251, 0.010627925395965576] score 4.864599081633649\n",
      "saved history to ../../data/results/history_gridsearch3_smalldualencoder_dualfacenet_h400_ed3triplet_decoder__st100_a40_g20_std2_ad2_gd23.csv\n",
      "epoch 2\n",
      "train loss 6.228239910943167 train embed loss 0.0 train accuracy [0.197, 0.557, 0.737]\n",
      "val loss 6.023317794799805 val_embed_loss 0.0 val accuracy [0.238, 0.629, 0.84] val f1 [0.116, 0.286, 0.417] disparities [0.5376233458518982, 0.9215865135192871, 0.03547060489654541] score 4.271436322157553\n",
      "saving model\n",
      "saved history to ../../data/results/history_gridsearch3_smalldualencoder_dualfacenet_h400_ed3triplet_decoder__st100_a40_g20_std2_ad2_gd23.csv\n",
      "epoch 3\n",
      "train loss 6.180983426619549 train embed loss 0.0 train accuracy [0.225, 0.563, 0.736]\n",
      "val loss 5.958181133270264 val_embed_loss 0.0 val accuracy [0.265, 0.636, 0.854] val f1 [0.127, 0.281, 0.423] disparities [0.5225974321365356, 0.9528864622116089, 0.12976229190826416] score 4.262509452992369\n",
      "saved history to ../../data/results/history_gridsearch3_smalldualencoder_dualfacenet_h400_ed3triplet_decoder__st100_a40_g20_std2_ad2_gd23.csv\n",
      "epoch 4\n",
      "train loss 6.166443289542685 train embed loss 0.0 train accuracy [0.224, 0.565, 0.735]\n",
      "val loss 5.913167037963867 val_embed_loss 0.0 val accuracy [0.273, 0.653, 0.862] val f1 [0.13, 0.308, 0.428] disparities [0.4922553598880768, 0.894317090511322, 0.021921873092651367] score 4.863061617922599\n",
      "saving model\n",
      "saved history to ../../data/results/history_gridsearch3_smalldualencoder_dualfacenet_h400_ed3triplet_decoder__st100_a40_g20_std2_ad2_gd23.csv\n",
      "epoch 5\n",
      "train loss 6.120140372490396 train embed loss 0.0 train accuracy [0.25, 0.581, 0.745]\n",
      "val loss 5.8750292015075685 val_embed_loss 0.0 val accuracy [0.282, 0.647, 0.861] val f1 [0.136, 0.292, 0.428] disparities [0.6016175746917725, 0.9444300532341003, 0.05529606342315674] score 4.505695016655019\n",
      "saved history to ../../data/results/history_gridsearch3_smalldualencoder_dualfacenet_h400_ed3triplet_decoder__st100_a40_g20_std2_ad2_gd23.csv\n",
      "epoch 6\n",
      "train loss 6.102650043915729 train embed loss 0.0 train accuracy [0.244, 0.592, 0.745]\n",
      "val loss 5.846630325317383 val_embed_loss 0.0 val accuracy [0.287, 0.665, 0.878] val f1 [0.139, 0.308, 0.437] disparities [0.45458438992500305, 0.9082362651824951, 0.0011522173881530762] score 5.034240231044961\n",
      "saving model\n",
      "saved history to ../../data/results/history_gridsearch3_smalldualencoder_dualfacenet_h400_ed3triplet_decoder__st100_a40_g20_std2_ad2_gd23.csv\n",
      "epoch 7\n",
      "train loss 6.072314710033183 train embed loss 0.0 train accuracy [0.251, 0.59, 0.751]\n",
      "val loss 5.833794250488281 val_embed_loss 0.0 val accuracy [0.305, 0.652, 0.874] val f1 [0.143, 0.306, 0.432] disparities [0.4864386320114136, 0.9101287722587585, 0.11778426170349121] score 4.956958404048606\n",
      "saved history to ../../data/results/history_gridsearch3_smalldualencoder_dualfacenet_h400_ed3triplet_decoder__st100_a40_g20_std2_ad2_gd23.csv\n",
      "epoch 8\n",
      "train loss 6.050215073994228 train embed loss 0.0 train accuracy [0.265, 0.594, 0.766]\n",
      "val loss 5.828878993988037 val_embed_loss 0.0 val accuracy [0.296, 0.655, 0.873] val f1 [0.146, 0.309, 0.433] disparities [0.4687727391719818, 0.9033401012420654, 0.12773019075393677] score 4.894810044869509\n",
      "saved history to ../../data/results/history_gridsearch3_smalldualencoder_dualfacenet_h400_ed3triplet_decoder__st100_a40_g20_std2_ad2_gd23.csv\n",
      "epoch 9\n",
      "train loss 6.041916915348598 train embed loss 0.0 train accuracy [0.257, 0.601, 0.766]\n",
      "val loss 5.84559476852417 val_embed_loss 0.0 val accuracy [0.293, 0.639, 0.875] val f1 [0.142, 0.289, 0.435] disparities [0.5666161775588989, 0.9411633014678955, 0.07949507236480713] score 4.665200875006735\n",
      "saved history to ../../data/results/history_gridsearch3_smalldualencoder_dualfacenet_h400_ed3triplet_decoder__st100_a40_g20_std2_ad2_gd23.csv\n",
      "epoch 10\n",
      "train loss 6.0297819838231925 train embed loss 0.0 train accuracy [0.271, 0.588, 0.767]\n",
      "val loss 5.814080791473389 val_embed_loss 0.0 val accuracy [0.306, 0.658, 0.882] val f1 [0.148, 0.296, 0.439] disparities [0.4637373983860016, 0.9526675343513489, 0.017831087112426758] score 4.970650079868262\n",
      "saved history to ../../data/results/history_gridsearch3_smalldualencoder_dualfacenet_h400_ed3triplet_decoder__st100_a40_g20_std2_ad2_gd23.csv\n",
      "epoch 11\n",
      "train loss 6.0290388769033 train embed loss 0.0 train accuracy [0.267, 0.599, 0.769]\n",
      "val loss 5.834449672698975 val_embed_loss 0.0 val accuracy [0.288, 0.656, 0.874] val f1 [0.144, 0.299, 0.433] disparities [0.520190417766571, 0.9400112628936768, 0.13919895887374878] score 4.580442383616197\n",
      "saved history to ../../data/results/history_gridsearch3_smalldualencoder_dualfacenet_h400_ed3triplet_decoder__st100_a40_g20_std2_ad2_gd23.csv\n",
      "epoch 12\n",
      "train loss 6.005500316619873 train embed loss 0.0 train accuracy [0.275, 0.598, 0.773]\n",
      "val loss 5.811642246246338 val_embed_loss 0.0 val accuracy [0.302, 0.654, 0.877] val f1 [0.15, 0.313, 0.437] disparities [0.6013333201408386, 0.8855096101760864, 0.008112490177154541] score 5.0847851985834\n",
      "saving model\n",
      "saved history to ../../data/results/history_gridsearch3_smalldualencoder_dualfacenet_h400_ed3triplet_decoder__st100_a40_g20_std2_ad2_gd23.csv\n",
      "epoch 13\n",
      "train loss 6.004860153003615 train embed loss 0.0 train accuracy [0.275, 0.602, 0.776]\n",
      "val loss 5.786555290222168 val_embed_loss 0.0 val accuracy [0.318, 0.656, 0.89] val f1 [0.157, 0.313, 0.444] disparities [0.5766666531562805, 0.8704215884208679, 0.013114392757415771] score 5.368369724405952\n",
      "saving model\n",
      "saved history to ../../data/results/history_gridsearch3_smalldualencoder_dualfacenet_h400_ed3triplet_decoder__st100_a40_g20_std2_ad2_gd23.csv\n",
      "epoch 14\n",
      "train loss 5.977474018019073 train embed loss 0.0 train accuracy [0.284, 0.603, 0.784]\n",
      "val loss 5.784837055206299 val_embed_loss 0.0 val accuracy [0.304, 0.665, 0.893] val f1 [0.155, 0.312, 0.444] disparities [0.6146667003631592, 0.9166490435600281, 0.04625362157821655] score 4.8984195953902905\n",
      "saved history to ../../data/results/history_gridsearch3_smalldualencoder_dualfacenet_h400_ed3triplet_decoder__st100_a40_g20_std2_ad2_gd23.csv\n",
      "epoch 15\n",
      "train loss 5.980913965069518 train embed loss 0.0 train accuracy [0.283, 0.604, 0.779]\n",
      "val loss 5.784400863647461 val_embed_loss 0.0 val accuracy [0.309, 0.663, 0.9] val f1 [0.156, 0.313, 0.448] disparities [0.5633333325386047, 0.9107503294944763, 0.05462074279785156] score 5.064249978333925\n",
      "saved history to ../../data/results/history_gridsearch3_smalldualencoder_dualfacenet_h400_ed3triplet_decoder__st100_a40_g20_std2_ad2_gd23.csv\n",
      "epoch 16\n",
      "train loss 5.966174398149763 train embed loss 0.0 train accuracy [0.286, 0.611, 0.789]\n",
      "val loss 5.753971881866455 val_embed_loss 0.0 val accuracy [0.313, 0.666, 0.894] val f1 [0.154, 0.315, 0.445] disparities [0.4864155948162079, 0.8961120247840881, 0.05944037437438965] score 5.24736372987298\n",
      "saved history to ../../data/results/history_gridsearch3_smalldualencoder_dualfacenet_h400_ed3triplet_decoder__st100_a40_g20_std2_ad2_gd23.csv\n",
      "epoch 17\n",
      "train loss 5.971686363220215 train embed loss 0.0 train accuracy [0.282, 0.61, 0.79]\n",
      "val loss 5.762763118743896 val_embed_loss 0.0 val accuracy [0.311, 0.659, 0.892] val f1 [0.16, 0.305, 0.443] disparities [0.5746666193008423, 0.9334885478019714, 0.12315833568572998] score 4.821702792327666\n",
      "saved history to ../../data/results/history_gridsearch3_smalldualencoder_dualfacenet_h400_ed3triplet_decoder__st100_a40_g20_std2_ad2_gd23.csv\n",
      "epoch 18\n",
      "train loss 5.945042488526325 train embed loss 0.0 train accuracy [0.292, 0.614, 0.79]\n",
      "val loss 5.77385009765625 val_embed_loss 0.0 val accuracy [0.329, 0.657, 0.879] val f1 [0.163, 0.296, 0.435] disparities [0.6426666975021362, 0.9493370056152344, 0.1948491930961609] score 4.604286935066454\n",
      "saved history to ../../data/results/history_gridsearch3_smalldualencoder_dualfacenet_h400_ed3triplet_decoder__st100_a40_g20_std2_ad2_gd23.csv\n",
      "epoch 19\n",
      "train loss 5.954272625397663 train embed loss 0.0 train accuracy [0.286, 0.615, 0.786]\n",
      "val loss 5.755691814422607 val_embed_loss 0.0 val accuracy [0.318, 0.673, 0.893] val f1 [0.163, 0.32, 0.444] disparities [0.5213333368301392, 0.912687361240387, 0.068817138671875] score 5.167116582415042\n",
      "saved history to ../../data/results/history_gridsearch3_smalldualencoder_dualfacenet_h400_ed3triplet_decoder__st100_a40_g20_std2_ad2_gd23.csv\n",
      "___________\n",
      "5.368369724405952 [0.31777777075767516, 0.6558222007751465, 0.8903999733924866] ../../data/models/gridsearch3_smalldualencoder_dualfacenet_h400_ed3triplet_decoder__st100_a40_g20_std2_ad2_gd2_rbalancedembed0.0_lw2-1-1_noise01 0 default\n",
      "______________\n",
      "(4869, 167)\n",
      "(1209, 167)\n",
      "model being saved to ../../data/models/gridsearch3_smalldualencoder_dualfacenet_h400_ed3triplet_decoder__st100_a40_g20_std2_ad2_gd2_rbalancedsoftembed0.11_lw2-1-1_noise01\n",
      "epoch 0\n",
      "train loss 6.533442755134738 train embed loss 0.13825720259729696 train accuracy [0.162, 0.428, 0.669]\n",
      "val loss 6.201227741241455 val_embed_loss 0.1207000832259655 val accuracy [0.19, 0.616, 0.847] val f1 [0.077, 0.304, 0.42] disparities [0.653401792049408, 0.672836184501648, 0.05627685785293579] score 4.6175722890168025\n",
      "saved history to ../../data/results/history_gridsearch3_smalldualencoder_dualfacenet_h400_ed3triplet_decoder__st100_a40_g20_std2_ad2_gd23.csv\n",
      "epoch 1\n",
      "train loss 6.323664159190898 train embed loss 0.14358307850756208 train accuracy [0.202, 0.526, 0.724]\n",
      "val loss 6.100107669830322 val_embed_loss 0.11588583856821061 val accuracy [0.248, 0.6, 0.836] val f1 [0.123, 0.301, 0.416] disparities [0.47501879930496216, 0.7144291996955872, 0.14464223384857178] score 5.020096097706084\n",
      "saved history to ../../data/results/history_gridsearch3_smalldualencoder_dualfacenet_h400_ed3triplet_decoder__st100_a40_g20_std2_ad2_gd23.csv\n",
      "epoch 2\n",
      "train loss 6.221807338753525 train embed loss 0.1477791662422978 train accuracy [0.23, 0.551, 0.741]\n",
      "val loss 5.949668579101562 val_embed_loss 0.12562453448772432 val accuracy [0.305, 0.618, 0.885] val f1 [0.16, 0.308, 0.44] disparities [0.4829002022743225, 0.6959159970283508, 0.0476527214050293] score 5.925358912886208\n",
      "saving model\n",
      "saved history to ../../data/results/history_gridsearch3_smalldualencoder_dualfacenet_h400_ed3triplet_decoder__st100_a40_g20_std2_ad2_gd23.csv\n",
      "epoch 3\n",
      "train loss 6.1538543409230755 train embed loss 0.14487781869817754 train accuracy [0.244, 0.556, 0.761]\n",
      "val loss 5.881534767150879 val_embed_loss 0.11582337006926537 val accuracy [0.315, 0.635, 0.882] val f1 [0.168, 0.304, 0.437] disparities [0.5980952978134155, 0.7596169710159302, 0.11410033702850342] score 5.549804362786005\n",
      "saved history to ../../data/results/history_gridsearch3_smalldualencoder_dualfacenet_h400_ed3triplet_decoder__st100_a40_g20_std2_ad2_gd23.csv\n",
      "epoch 4\n",
      "train loss 6.120138596515266 train embed loss 0.14036453857409711 train accuracy [0.248, 0.561, 0.757]\n",
      "val loss 5.878154029846192 val_embed_loss 0.11831764161586761 val accuracy [0.29, 0.658, 0.869] val f1 [0.154, 0.321, 0.432] disparities [0.44211190938949585, 0.7842193245887756, 0.06556332111358643] score 5.485570079132401\n",
      "saved history to ../../data/results/history_gridsearch3_smalldualencoder_dualfacenet_h400_ed3triplet_decoder__st100_a40_g20_std2_ad2_gd23.csv\n",
      "epoch 5\n",
      "train loss 6.093873145628948 train embed loss 0.13811665963457556 train accuracy [0.257, 0.582, 0.76]\n",
      "val loss 5.811247291564942 val_embed_loss 0.10316225498914719 val accuracy [0.282, 0.667, 0.893] val f1 [0.142, 0.317, 0.444] disparities [0.42128071188926697, 0.8444773554801941, 0.04335582256317139] score 5.256258594577524\n",
      "saved history to ../../data/results/history_gridsearch3_smalldualencoder_dualfacenet_h400_ed3triplet_decoder__st100_a40_g20_std2_ad2_gd23.csv\n",
      "epoch 6\n",
      "train loss 6.042886359351022 train embed loss 0.13722771573431639 train accuracy [0.267, 0.6, 0.778]\n",
      "val loss 5.814531650543213 val_embed_loss 0.12252993822097778 val accuracy [0.297, 0.659, 0.892] val f1 [0.16, 0.309, 0.444] disparities [0.4813809394836426, 0.8830664753913879, 0.03682905435562134] score 5.190970993092093\n",
      "saved history to ../../data/results/history_gridsearch3_smalldualencoder_dualfacenet_h400_ed3triplet_decoder__st100_a40_g20_std2_ad2_gd23.csv\n",
      "epoch 7\n",
      "train loss 6.027755095034229 train embed loss 0.13820730598301303 train accuracy [0.279, 0.595, 0.783]\n",
      "val loss 5.816290187835693 val_embed_loss 0.12168340235948563 val accuracy [0.287, 0.677, 0.884] val f1 [0.15, 0.322, 0.438] disparities [0.46550917625427246, 0.8566558361053467, 0.10937321186065674] score 5.1008658755625245\n",
      "saved history to ../../data/results/history_gridsearch3_smalldualencoder_dualfacenet_h400_ed3triplet_decoder__st100_a40_g20_std2_ad2_gd23.csv\n",
      "epoch 8\n",
      "train loss 6.00264607157026 train embed loss 0.1368481976797386 train accuracy [0.281, 0.603, 0.781]\n",
      "val loss 5.794918537139893 val_embed_loss 0.12475578695535659 val accuracy [0.3, 0.665, 0.886] val f1 [0.146, 0.305, 0.44] disparities [0.47334596514701843, 0.9118886590003967, 0.06911903619766235] score 5.031223228009392\n",
      "saved history to ../../data/results/history_gridsearch3_smalldualencoder_dualfacenet_h400_ed3triplet_decoder__st100_a40_g20_std2_ad2_gd23.csv\n",
      "___________\n",
      "5.925358912886208 [0.3045333254337311, 0.6179555368423462, 0.8847999811172486] ../../data/models/gridsearch3_smalldualencoder_dualfacenet_h400_ed3triplet_decoder__st100_a40_g20_std2_ad2_gd2_rbalancedsoftembed0.11_lw2-1-1_noise01 0.1 softmax\n",
      "______________\n",
      "(4869, 167)\n",
      "(1209, 167)\n",
      "model being saved to ../../data/models/gridsearch3_smalldualencoder_dualfacenet_h400_ed3triplet_decoder__st100_a40_g20_std2_ad2_gd2_rbalancedembed0.11_lw2-1-1_noise01\n",
      "epoch 0\n",
      "train loss 6.569287095751081 train embed loss 0.13333456142216313 train accuracy [0.12, 0.409, 0.671]\n",
      "val loss 6.2754401588439945 val_embed_loss 0.10925784781575203 val accuracy [0.207, 0.599, 0.837] val f1 [0.099, 0.238, 0.414] disparities [0.6410476565361023, 0.5544173121452332, 0.11130833625793457] score 4.987987159831818\n",
      "saved history to ../../data/results/history_gridsearch3_smalldualencoder_dualfacenet_h400_ed3triplet_decoder__st100_a40_g20_std2_ad2_gd23.csv\n",
      "epoch 1\n"
     ]
    }
   ],
   "source": [
    "def save_train_history(model,history,root=''):\n",
    "    model_name = model.get_identifier()\n",
    "    \n",
    "    df = pd.DataFrame(history)\n",
    "    df['model'] = model_name\n",
    "    string = root + 'results/history_' + model_name + '3.csv'\n",
    "    df.to_csv(string,index=False)\n",
    "    print('saved history to',string)\n",
    "    return df, string\n",
    "\n",
    "def subclass_accuracy(ypred,y,nclass=None):\n",
    "    if nclass is None:\n",
    "        nclass = len(torch.unique(y))\n",
    "        classes = torch.unique(y)\n",
    "    else:\n",
    "        classes = torch.Tensor([i for i in range(nclass)]).float()\n",
    "    results = torch.zeros(nclass)\n",
    "    if ypred.ndim > 1:\n",
    "        ypred = torch.argmax(ypred,1).long()\n",
    "    for i,c in enumerate(torch.unique(y)):\n",
    "        yy = (y == c)\n",
    "        yypred = (ypred == c)\n",
    "        good = torch.logical_and(yy,yypred).float().sum()/yy.float().sum()\n",
    "        results[i] = good\n",
    "    return results\n",
    "\n",
    "def comp_score(accs,disp,csizes):\n",
    "    score = 0\n",
    "    for acc,disp,size in zip(accs,disp,csizes):\n",
    "        score += size*(acc)*(1-disp**(size/2))\n",
    "    return score\n",
    "\n",
    "def train_model(model,\n",
    "                train_df,\n",
    "                validation_df,\n",
    "                root,\n",
    "                epochs=300,\n",
    "                lr=.001,\n",
    "                batch_size=200,\n",
    "                patience = 20,\n",
    "                loss_weights = [2,1,.5],\n",
    "                save_path=None,\n",
    "                histogram =False,\n",
    "                upsample=False,\n",
    "                random_upsample=True,\n",
    "                softmax_upsample_weights=False,\n",
    "                upsample_validation=False,\n",
    "                random_upsample_validation=False,\n",
    "                embedding_loss_weight = 1,\n",
    "                classification_loss_weight = 1,\n",
    "                starting_loss=100000,\n",
    "                bias_weight_transform=None,\n",
    "                skintone_regression=False,\n",
    "                noise=.05,\n",
    "                skintone_patch_anchor_prob=0,\n",
    "                run_name = '',\n",
    "                **kwargs,\n",
    "               ):\n",
    "    pretraining = classification_loss_weight <= .00001\n",
    "    if save_path is None:\n",
    "        save_path = root + 'models/'+ run_name + model.get_identifier() \n",
    "        if random_upsample:\n",
    "            \n",
    "            save_path += '_rbalanced'\n",
    "            if softmax_upsample_weights:\n",
    "                save_path += 'soft'\n",
    "        elif upsample:\n",
    "            save_path += '_balanced'\n",
    "        if pretraining:\n",
    "            save_path += '_pretrain'\n",
    "        else:\n",
    "            save_path += 'embed' + str(np.round(embedding_loss_weight/classification_loss_weight,2))\n",
    "    if skintone_patch_anchor_prob > 0:\n",
    "        save_path += '_spab'+str(skintone_patch_anchor_prob).replace('.','')\n",
    "    save_path += '_lw' + '-'.join([str(l) for l in loss_weights])\n",
    "    save_path += '_noise' + str(noise).replace('0.','').replace('.','')\n",
    "    if upsample:\n",
    "        patience = int(patience/5) + 1\n",
    "    class_sizes = [10,4,2]\n",
    "    if bias_weight_transform is not None:\n",
    "        weight_cols = [c for c in train_df.columns if '_bias' in c]\n",
    "        tdf = train_df.copy()\n",
    "        vdf = validation_df.copy()\n",
    "        for col in weight_cols:\n",
    "            tdf[col] = tdf[col].apply(bias_weight_transform)\n",
    "            vdf[col] = vdf[col].apply(bias_weight_transform)\n",
    "        train_df = tdf\n",
    "        validation_df = vdf\n",
    "    train_loader = TripletFaceGenerator(train_df,Constants.data_root,\n",
    "                                 batch_size=batch_size,\n",
    "                                 upsample=upsample,\n",
    "                                 random_upsample=random_upsample,\n",
    "                                 skintone_patch_anchor_prob=skintone_patch_anchor_prob,\n",
    "                                 softmax=softmax_upsample_weights,\n",
    "                                 noise_sigma=noise,\n",
    "                                 **kwargs)\n",
    "    validation_loader = TripletFaceGenerator(validation_df,Constants.data_root,\n",
    "                                      validation=True,\n",
    "                                      batch_size=batch_size,\n",
    "                                      upsample=upsample_validation,\n",
    "                                     random_upsample=random_upsample_validation,\n",
    "                                     softmax=softmax_upsample_weights,\n",
    "                                     skintone_patch_anchor_prob=0,\n",
    "                                     noise_sigma=noise,\n",
    "                                      **kwargs)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.train(True)\n",
    "    \n",
    "    cel = torch.nn.CrossEntropyLoss()\n",
    "    format_y = lambda y: y.to(device)\n",
    "    regression_loss = torch.nn.MSELoss()\n",
    "    triplet_loss = torch.nn.TripletMarginLoss()\n",
    "#     embedding_optimizer = torch.optim.Adam(model.encoder.parameters(), lr=lr)\n",
    "#     decoder_optimizer = torch.optim.Adam(model.decoder.parameters(), lr=lr)\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "    \n",
    "    \n",
    "    def format_batch(inputs,grad=True):\n",
    "        xb = []\n",
    "        for xin in inputs:\n",
    "            xin = xin.to(device)\n",
    "            xin.requires_grad_(grad)\n",
    "            xb.append(xin)\n",
    "        return xb\n",
    "    \n",
    "    def embedding_step(m,xbatch): \n",
    "        base = m.encoder(xbatch[0])\n",
    "        positive = m.encoder(xbatch[1])\n",
    "        negative = m.encoder(xbatch[2])\n",
    "        loss = triplet_loss(base,positive,negative)\n",
    "        loss = torch.mul(loss,embedding_loss_weight)\n",
    "        return base,loss\n",
    "    \n",
    "    \n",
    "    def classifier_step(m,embedding,ytrue):\n",
    "        outputs = m.decoder(embedding)\n",
    "        if not skintone_regression:\n",
    "            losses = [cel(ypred.float(),format_y(y.long())) for y,ypred in zip(ytrue,outputs)]\n",
    "            l1 = torch.mul(loss_weights[0],losses[0])\n",
    "            l2 =  torch.mul(loss_weights[1],losses[1])\n",
    "            l3 =  torch.mul(loss_weights[2],losses[2])\n",
    "        else:\n",
    "            l1 = torch.mul( regression_loss(outputs[0].float().view(-1),format_y(ytrue[0].float())), loss_weights[0])\n",
    "            l2 = torch.mul( cel(outputs[1].float(),format_y(ytrue[1].long())), loss_weights[1])\n",
    "            l3 = torch.mul(cel(outputs[2].float(),format_y(ytrue[2].long())), loss_weights[2])\n",
    "        total_losses = l1 + l2 + l3\n",
    "        total_loss = torch.mul(total_losses,classification_loss_weight)\n",
    "        return outputs,total_losses\n",
    "        \n",
    "    def train_epoch(model):\n",
    "        running_loss = 0\n",
    "        running_embed_loss = 0\n",
    "        running_accuracy = [0,0,0]\n",
    "        curr_loss = 0\n",
    "        count = 0\n",
    "        for i, [x_batch, y_batch] in enumerate(train_loader):\n",
    "            x_batch = format_batch(x_batch)\n",
    "            optimizer.zero_grad()\n",
    "            embedding,embedding_loss = embedding_step(model, x_batch)\n",
    "            if pretraining:\n",
    "                classification_loss = embedding_loss - embedding_loss\n",
    "            else:\n",
    "                outputs, classification_loss = classifier_step(model,embedding,y_batch)\n",
    "            total_loss = classification_loss + embedding_loss\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += classification_loss.item()\n",
    "            running_embed_loss += embedding_loss.item()\n",
    "            print('curr loss class',classification_loss.item(),'embed', embedding_loss.item(), 'step',i,' | ',end='\\r')\n",
    "            count += 1\n",
    "            if not pretraining:\n",
    "                with torch.no_grad():\n",
    "                    for i,(y,ypred) in enumerate(zip(y_batch,outputs)):\n",
    "                        accuracy = Utils.categorical_accuracy(ypred.float(),format_y(y))\n",
    "                        running_accuracy[i] += accuracy.item()\n",
    "        return running_loss/count,running_embed_loss/count, [a/count for a in running_accuracy]\n",
    "    \n",
    "    def val_epoch(model):\n",
    "        running_loss = 0\n",
    "        running_embed_loss = 0\n",
    "        running_accuracy = [0,0,0]\n",
    "        running_f1 = [0,0,0]\n",
    "        running_subclass_acc = [None,None,None]\n",
    "        count = 0\n",
    "        with torch.no_grad():\n",
    "            for i, [x_batch, y_batch] in enumerate(validation_loader):\n",
    "                x_batch = format_batch(x_batch,grad=False)\n",
    "                embedding,embedding_loss = embedding_step(model, x_batch)\n",
    "                if pretraining:\n",
    "                    classification_loss = embedding_loss - embedding_loss\n",
    "                else:\n",
    "                    outputs, classification_loss = classifier_step(model,embedding,y_batch)\n",
    "                running_loss += classification_loss.item()\n",
    "                running_embed_loss += embedding_loss.item()\n",
    "                count += 1\n",
    "                if not pretraining:\n",
    "                    for i,(y,ypred) in enumerate(zip(y_batch, outputs)):\n",
    "                        accuracy = Utils.categorical_accuracy(ypred.float(),format_y(y))\n",
    "                        f1, precision, recall = Utils.macro_f1(torch.argmax(ypred.float(),axis=1),format_y(y))\n",
    "                        subclass_acc = subclass_accuracy(ypred.float(),format_y(y),class_sizes[i])\n",
    "                        if running_subclass_acc[i] is None:\n",
    "                            running_subclass_acc[i] = subclass_acc\n",
    "                        else:\n",
    "                            running_subclass_acc[i] += subclass_acc\n",
    "                        running_accuracy[i] += accuracy.item()\n",
    "                        running_f1[i] += f1.item()\n",
    "        running_subclass_acc = [torch.mul(r,1/count) for r in running_subclass_acc]\n",
    "        disparities = [(torch.max(v) - torch.min(v)).item() for v in running_subclass_acc]\n",
    "        return running_loss/count,running_embed_loss/count, [a/count for a in running_accuracy], [f/count for f in running_f1],disparities\n",
    "    shorten = lambda array: [np.round(a, 3) for a in array]\n",
    "    \n",
    "    best_val_res = {}\n",
    "    best_val_loss=1000000\n",
    "    best_score = 0\n",
    "    steps_since_improvement = 0\n",
    "    hist = []\n",
    "    best_weights = model.state_dict()\n",
    "    print('model being saved to',save_path)\n",
    "    for epoch in range(epochs):\n",
    "        print('epoch',epoch)\n",
    "        model.train()\n",
    "        avg_loss,avg_embed_loss, avg_acc = train_epoch(model)\n",
    "        print('train loss', avg_loss,'train embed loss',avg_embed_loss, 'train accuracy', shorten(avg_acc))\n",
    "        model.eval()\n",
    "        val_loss,val_embed_loss, val_acc, val_f1, val_disp = val_epoch(model)\n",
    "        val_score = comp_score(val_acc,val_disp,class_sizes)\n",
    "        if pretraining:\n",
    "            val_loss = val_embed_loss\n",
    "        print('val loss', val_loss, 'val_embed_loss', val_embed_loss, \n",
    "              'val accuracy', shorten(val_acc), 'val f1', shorten(val_f1),\n",
    "                 'disparities',val_disp, 'score', val_score,\n",
    "             )\n",
    "        #don't save immediately in case I cancel training\n",
    "        if best_score < val_score and epoch > 1:\n",
    "            torch.save(model,save_path)\n",
    "            torch.save(model.state_dict(),save_path+'_states')\n",
    "            print('saving model')\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "            best_val_loss = val_loss\n",
    "            best_score = val_score\n",
    "            best_val_res = {'accuracy': val_acc, 'f1': val_f1, 'disparity': val_disp,'score':best_score,'loss':val_loss}\n",
    "            steps_since_improvement = 0\n",
    "        else:\n",
    "            steps_since_improvement += 1\n",
    "        \n",
    "        hist_entry = {\n",
    "            'epoch': epoch,\n",
    "            'train_loss': avg_loss,\n",
    "            'train_acc':avg_acc,\n",
    "            'val_loss':val_loss,\n",
    "            'val_acc': val_acc,\n",
    "            'score': val_score,\n",
    "            'disp': val_disp,\n",
    "            'lr': lr,\n",
    "            'loss_weights': '_'.join([str(l) for l in loss_weights])\n",
    "        }\n",
    "        hist.append(hist_entry)\n",
    "        save_train_history(model,hist,root=root)\n",
    "        if steps_since_improvement > patience:\n",
    "            break\n",
    "    model.load_state_dict(best_weights)\n",
    "    return model,hist,best_val_res,save_path\n",
    "\n",
    "    \n",
    "models = [\n",
    "#      lambda : TripletModel2(encoder=TripletFacenetEncoder(embedding_dropout=.1),decoder=TripletFacenetClassifier(400,st_dims=[20],age_dims=[8],gender_dims=[4]),name='gridsearch_sm0ll'),\n",
    "#     lambda : TripletModel2(encoder=TripletFacenetEncoder(embedding_dropout=.1),decoder=TripletFacenetClassifier(400,st_dims=[100],age_dims=[40],gender_dims=[20]),name='gridsearch_small'),\n",
    "#     lambda : TripletModel2(encoder=TripletFacenetEncoder(embedding_dropout=.3),decoder=TripletFacenetClassifier(400,st_dims=[20],age_dims=[8],gender_dims=[4]),name='gridsearch_sm0ll'),\n",
    "    lambda : TripletModel2(encoder=TripletFacenetEncoder(embedding_dropout=.3),decoder=TripletFacenetClassifier(400,st_dims=[100],age_dims=[40],gender_dims=[20]),name='gridsearch3_small'),\n",
    "#     lambda : TripletModel2(encoder=TripletFacenetEncoder(embedding_dropout=.3),decoder=TripletFacenetClassifier(400,st_dims=[100],age_dims=[100],gender_dims=[100]),name='gridsearch_medium'),\n",
    "#     lambda : TripletModel2(encoder=TripletFacenetEncoder(embedding_dropout=.6),decoder=TripletFacenetClassifier(400,st_dims=[20],age_dims=[8],gender_dims=[4]),name='gridsearch_sm0ll'),\n",
    "#     lambda : TripletModel2(encoder=TripletFacenetEncoder(embedding_dropout=.6),decoder=TripletFacenetClassifier(400,st_dims=[100],age_dims=[40],gender_dims=[20]),name='gridsearch_small'),\n",
    "#     lambda : TripletModel2(encoder=TripletFacenetEncoder(embedding_dropout=.6),decoder=TripletFacenetClassifier(400,st_dims=[100],age_dims=[100],gender_dims=[100]),name='gridsearch_medium'),\n",
    "    #     lambda : TripletModel2(encoder=TripletFacenetEncoder(),name='gridsearch_baseline'),\n",
    "#     lambda : TripletModel2(encoder=TripletFacenetEncoder(base_model=ResNet18()),decoder=TripletFacenetClassifier(400,st_dims=[100],age_dims=[40],gender_dims=[20]),name='gridsearch3_resnetsmall'),\n",
    "#     lambda : TripletModel2(encoder=SimpleEncoder(),name='gridsearch_simple'),\n",
    "#     lambda : TripletModel2(encoder=FrozenDualEncoder(),name='gridsearch_frozen'),\n",
    "#     lambda : TripletModel2(encoder=SimpleEncoder(fine_tune=False),name='gridsearch_simplefrozen'),\n",
    "]\n",
    "\n",
    "results = []\n",
    "for embed_loss_weight in [0,.1]:\n",
    "    for stap in [0]:\n",
    "        for noise in [.01]:\n",
    "            for loss_weights in [[2,1,1]]:\n",
    "                for model in models:\n",
    "                    for wt in ['softmax','default']:\n",
    "                        if embed_loss_weight == 0 and wt != 'default':\n",
    "                            continue\n",
    "                        softmax = (wt == 'softmax')\n",
    "                        bias_weight_transform=None\n",
    "                        if wt == 'flat':\n",
    "                            bias_weight_transform = lambda x: int(x > 0)\n",
    "                        try:\n",
    "                            m,h,entry,mname = train_model(\n",
    "                                model(),\n",
    "                                train_labels,\n",
    "                                validation_labels,\n",
    "                                Constants.data_root,\n",
    "                                batch_size=50,\n",
    "                                embedding_loss_weight=embed_loss_weight,\n",
    "                                classification_loss_weight=1 - embed_loss_weight,\n",
    "                                lr=.0001,\n",
    "                                patience=5,\n",
    "                                softmax_upsample_weights=softmax,\n",
    "                                loss_weights=loss_weights,\n",
    "                                skintone_patch_anchor_prob=stap,\n",
    "                                skintone_regression=False,\n",
    "                                noise=noise,\n",
    "                            )\n",
    "                            entry['noise'] = noise\n",
    "                            entry['embed_weight'] = embed_loss_weight\n",
    "                            entry['model'] = mname\n",
    "                            entry['st_patch_prob'] = stap\n",
    "                            entry['softmax'] = softmax\n",
    "                            entry['weight_type'] = wt\n",
    "                            entry['loss_weights'] = loss_weights\n",
    "                            results.append(entry)\n",
    "                            print('___________')\n",
    "                            print(entry['score'],\n",
    "                                  entry['accuracy'],\n",
    "                                  entry['model'],\n",
    "                                  entry['embed_weight'],\n",
    "                                  wt,\n",
    "                                 )\n",
    "                            print('______________')\n",
    "                            pd.DataFrame(results).to_csv(Constants.result_folder + '_weight_gridsearch_presentation.csv')\n",
    "                        except Exception as e:\n",
    "                            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7e1486-f1e6-4e48-92f0-9e3e72f1896b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results).sort_values('score',ascending=False).model.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbcaf49-3a9c-45c1-93e7-888fef3dbc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = TripletModel2(encoder=TripletFacenetEncoder(),name='gridsearch_baseline')\n",
    "test.encoder.embedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a69f57-285b-4807-8b1f-c1f5217a6a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('lol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427653cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
